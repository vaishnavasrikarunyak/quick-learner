import gradio as gr
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# --- Model Loading ---
# This function loads the model once when the application starts to avoid
# reloading it for every request. Using torch.bfloat16 and bitsandbytes
# for quantization helps reduce memory usage.

def load_model():
    """Loads the model pipeline for text generation."""
    try:
        model_name = "ibm-granite/granite-3.3-2b-instruct"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
        )
        pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
        return pipe
    except Exception as e:
        return f"Error loading model: {e}"

# Load the model globally when the app starts
pipe = load_model()

# --- Chatbot Logic ---
def chatbot_response(user_input, user_type):
    """
    Generates a personalized financial guidance response.
    
    The function constructs a system prompt to instruct the model to
    adjust its tone and complexity based on the user's type (Student or Professional).
    """
    if isinstance(pipe, str):
        return pipe # Return error message if model failed to load

    # Create a system message to guide the model's persona
    system_prompt = (
        "You are an intelligent personal finance chatbot that provides guidance on savings, "
        "taxes, and investments. Your goal is to help the user improve their financial literacy. "
        f"Your user is a {user_type.lower()}. Please tailor your response to their level of "
        "understanding and provide actionable, relevant advice.\n\n"
    )

    # Differentiate the tone based on the user type
    if user_type == "Student":
        system_prompt += (
            "For a student, use simple, encouraging, and easy-to-understand language. "
            "Focus on concepts like managing a budget, saving for a short-term goal, or understanding "
            "the basics of a first job's income and taxes. Avoid complex financial jargon.\n\n"
        )
    elif user_type == "Professional":
        system_prompt += (
            "For a professional, use more detailed and comprehensive language. "
            "Focus on concepts like retirement planning, investment strategies, tax optimization, "
            "and more complex financial products. You can use financial terminology and assume "
            "a higher level of background knowledge.\n\n"
        )

    # Combine the system prompt and user's query
    full_prompt = system_prompt + f"User's question: {user_input}"

    # Prepare the message in the correct format for the model
    messages = [{"role": "user", "content": full_prompt}]

    # Generate the response
    try:
        outputs = pipe(messages, max_new_tokens=256, temperature=0.7)
        # The response is typically structured with the model's output in the last part
        response_text = outputs[0]['generated_text']
        # Extract the model's response, removing the initial prompt and any incomplete sentences
        generated_reply = response_text[len(full_prompt):].strip()

        # Simple check to ensure a full sentence is returned
        if not generated_reply.endswith(('.', '!', '?')):
            generated_reply = generated_reply.rsplit(' ', 1)[0] + '...'

        return generated_reply
    except Exception as e:
        return f"An error occurred while generating the response: {e}"

# --- Gradio Interface ---
# Set up the Gradio interface with an interactive chat UI.
# The `chatbot_response` function is connected to the interface.
with gr.Blocks(title="Personal Finance Chatbot") as demo:
    gr.Markdown(
        """
        # Personal Finance Chatbot
        Intelligent Guidance for Savings, Taxes, and Investments.
        
        This chatbot leverages IBM's generative AI to provide personalized financial guidance.
        **Select your user type below to get started!**
        """
    )
